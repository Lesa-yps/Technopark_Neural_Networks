{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tjJocIf2SVzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvgXC6YxSm12"
      },
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "outputs": [],
      "source": [
        "EPS = 1e-5\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        return torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return torch.multinomial(probs, 1).item()\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int\n",
        "    ) -> str:\n",
        "        # токенизация промпта\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        # (sequence, score)\n",
        "        beams = [(input_ids, 0.0)]\n",
        "        \n",
        "        for _ in range(max_length - len(input_ids[0])):\n",
        "            new_beams = []\n",
        "            \n",
        "            for beam_seq, beam_score in beams:\n",
        "                # вычисление логитов для последнего токена\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(beam_seq)\n",
        "                    next_token_logits = outputs.logits[:, -1, :]\n",
        "                \n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                \n",
        "                # выбор топ-K кандидатов\n",
        "                topk_probs, topk_indices = torch.topk(probs[0], num_beams * 2)\n",
        "                \n",
        "                for prob, token_id in zip(topk_probs, topk_indices):\n",
        "                    new_seq = torch.cat([beam_seq[0], token_id.unsqueeze(0)]).unsqueeze(0)\n",
        "                    new_score = beam_score + torch.log(prob).item()\n",
        "                    new_beams.append((new_seq, new_score))\n",
        "            \n",
        "            # сортировка по score и выбор топ-K\n",
        "            new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "            beams = new_beams[:num_beams]\n",
        "            \n",
        "            all_finished = all(beam_seq[0, -1] == self.tokenizer.eos_token_id for beam_seq, _ in beams)\n",
        "            if all_finished:\n",
        "                break\n",
        "        \n",
        "        # выбор лучшей последовательности\n",
        "        best_sequence, best_score = max(beams, key=lambda x: x[1])\n",
        "        return self.tokenizer.decode(best_sequence[0], skip_special_tokens=True)\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        if temperature <= 0:\n",
        "            raise ValueError(\"Temperature must be positive\")\n",
        "        return logits / temperature\n",
        "\n",
        "    def _apply_top_k(self, logits: torch.Tensor, top_k: float = None) -> torch.Tensor:\n",
        "        if top_k is None or top_k >= self.vocab_size:\n",
        "            return logits\n",
        "        \n",
        "        # поиск k-го по величине значения для каждого примера\n",
        "        top_k_values = torch.topk(logits, top_k, dim=-1).values\n",
        "        kth_value = top_k_values[:, -1]\n",
        "        \n",
        "        # маска с True для всех значений меньше k-го\n",
        "        mask = logits < kth_value.unsqueeze(-1)\n",
        "        # замена маленьких по маске значения на -бесконечность\n",
        "        logits[mask] = float('-inf')\n",
        "    \n",
        "        return logits\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 1.0) -> torch.Tensor:\n",
        "        if top_p >= 1.0:\n",
        "            return logits\n",
        "        \n",
        "        # сортировка вероятностей\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "        \n",
        "        # маска для токенов вне top-p\n",
        "        sorted_mask = cumulative_probs > top_p\n",
        "        # сдвиг маски для вкл первого токена, превышающего порог\n",
        "        sorted_mask[:, 1:] = sorted_mask[:, :-1].clone()\n",
        "        sorted_mask[:, 0] = 0\n",
        "        \n",
        "        # восстановление исходного порядка и создание маски\n",
        "        mask = torch.zeros_like(logits, dtype=torch.bool)\n",
        "        mask.scatter_(1, sorted_indices, sorted_mask)\n",
        "        \n",
        "        # отфильтрованные логиты заменяются на -бесконечность\n",
        "        filtered_logits = logits.masked_fill(mask, float('-inf'))\n",
        "        return filtered_logits\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "        if strategy == \"beam_search\":\n",
        "            return self._beam_search_generate(prompt, max_length, num_beams)\n",
        "        \n",
        "        # токенизация промта\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        generated = input_ids.clone()\n",
        "\n",
        "        for _ in range(max_length - len(input_ids[0])):\n",
        "            # вычисление логитов для последнего токена\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(generated)\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            if abs(temperature) > EPS:\n",
        "                next_token_logits = self.apply_temperature(next_token_logits, temperature)\n",
        "            else:\n",
        "                strategy = \"greedy\"\n",
        "\n",
        "            if top_k > 0:\n",
        "                next_token_logits = self._apply_top_k(next_token_logits, top_k)\n",
        "\n",
        "            if top_p < 1.0:\n",
        "                next_token_logits = self._apply_top_p(next_token_logits, top_p)\n",
        "\n",
        "            if strategy == \"greedy\":\n",
        "                next_token_id = self.greedy_sampling(next_token_logits)\n",
        "            elif strategy == \"random\":\n",
        "                next_token_id = self.random_sampling(next_token_logits)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "            \n",
        "            # добавление токена к последовательности\n",
        "            next_token = torch.tensor([[next_token_id]])\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "            # достижение конца текста\n",
        "            if next_token_id == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.tokenizer.decode(generated[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aNUHC3UmSYd-"
      },
      "outputs": [],
      "source": [
        "# Продемонстрируйте результат работы `generate` при различных параметрах\n",
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Artificial intelligence has made great strides forward. This is\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Greedy sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence has made great strides forward. This is a great achievement, but it is not the only one.\n",
            "\n",
            "The next step is to\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(prompt, strategy=\"greedy\", max_length=30)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence has made great strides forward. This is partly because intelligent machines nowadays can be tweaked more rapidly than humans. Other problems may be much smaller\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(prompt, strategy=\"random\", max_length=30)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random sampling + temp=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence has made great strides forward. This is happens bacon∴ fil).\" >>> Once It AC Jobs Thomas Conglees Sal Meielve Dal Griffin\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(prompt, strategy=\"random\", temperature=2.0, max_length=30)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random sampling + temp=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence has made great strides forward. This is because it is able to find and understand patterns in the world and to find and understand patterns in\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(prompt, strategy=\"random\", temperature=0.5, max_length=30)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random sampling + top_k=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence has made great strides forward. This is a great thing for our nation,\" said the President.\n",
            "\n",
            "But there's another side to\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(prompt, strategy=\"random\", top_k=10, max_length=30)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random sampling + top_p=0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence has made great strides forward. This is true because in recent years, our methods have improved and we have more information to apply to more\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(prompt, strategy=\"random\", top_p=0.8, max_length=30)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beam search (num_beams=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence has made great strides forward. This is not to say that AI is going to be the next big thing, but it's clear that\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(prompt, strategy=\"beam_search\", max_length=30, num_beams=3)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Комбинации параметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence has made great strides forward. This is the first time in the history of the world that a single AI is able to predict the future, and this is a big step forward for the human race.\n",
            "\n",
            "The AI that we know\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(\n",
        "    prompt, \n",
        "    strategy=\"random\", \n",
        "    temperature=0.7, \n",
        "    top_k=8, \n",
        "    top_p=0.95, \n",
        "    max_length=50\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt1 = \"And they tell us that the leg is shorter than the hypotenuse. And I tell you: Enough!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "And they tell us that the leg is shorter than the hypotenuse. And I tell you: Enough!\n",
            "\n",
            "But they don't know what's happening to the other one, because you can see how far the leg will fall off in the ground, so you can't really tell what is actually going on here.\n",
            "\n",
            "The leg that's going up, you have the leg. You have the knee. You have the back of the thigh that's just above the elbow where it can\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(\n",
        "    prompt1, \n",
        "    strategy=\"random\", \n",
        "    temperature=1.5, \n",
        "    top_k=8, \n",
        "    max_length=100\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "And they tell us that the leg is shorter than the hypotenuse. And I tell you: Enough!\n",
            "\n",
            "I'm not saying that the leg is shorter than the hypotenuse. I'm saying that the leg is shorter than the hypotenuse.\n",
            "\n",
            "I'm not saying that the leg is shorter than the hypotenuse.\n",
            "\n",
            "I'm not saying that the leg is shorter than the hypotenuse.\n",
            "\n",
            "I'm not saying that the leg is shorter than the\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(\n",
        "    prompt1, \n",
        "    strategy=\"greedy\",\n",
        "    max_length=100\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "And they tell us that the leg is shorter than the hypotenuse. And I tell you: Enough!\n",
            "\n",
            "The other issue is that the final table shows an astonishing amount of subtle differences in surface area. The pentagonal paper covers only 3.5-inches across. Because the backboard is long, this leaves little room for slightly over 10 feet of board space. But it also leaves room for nearly 2 feet of gain. The foot-depth was 28 inches and not even 3.\n"
          ]
        }
      ],
      "source": [
        "result = model.generate(\n",
        "    prompt1, \n",
        "    strategy=\"random\", \n",
        "    temperature=1.2, \n",
        "    top_p=0.6, \n",
        "    max_length=100\n",
        ")\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
